{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d040ac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "from __future__ import division\n",
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ea6ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb7a58d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Calibration(object):\n",
    "    \"\"\"\n",
    "    This class calibrates the pupil detection algorithm by finding the\n",
    "    best binarization threshold value for the person and the webcam.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nb_frames = 20\n",
    "        self.thresholds_left = []\n",
    "        self.thresholds_right = []\n",
    "\n",
    "    def is_complete(self):\n",
    "        \"\"\"Returns true if the calibration is completed\"\"\"\n",
    "        return len(self.thresholds_left) >= self.nb_frames and len(self.thresholds_right) >= self.nb_frames\n",
    "\n",
    "    def threshold(self, side):\n",
    "        \"\"\"Returns the threshold value for the given eye.\n",
    "\n",
    "        Argument:\n",
    "            side: Indicates whether it's the left eye (0) or the right eye (1)\n",
    "        \"\"\"\n",
    "        if side == 0:\n",
    "            return int(sum(self.thresholds_left) / len(self.thresholds_left))\n",
    "        elif side == 1:\n",
    "            return int(sum(self.thresholds_right) / len(self.thresholds_right))\n",
    "\n",
    "    @staticmethod\n",
    "    def iris_size(frame):\n",
    "        \"\"\"Returns the percentage of space that the iris takes up on\n",
    "        the surface of the eye.\n",
    "\n",
    "        Argument:\n",
    "            frame (numpy.ndarray): Binarized iris frame\n",
    "        \"\"\"\n",
    "        frame = frame[5:-5, 5:-5]\n",
    "        height, width = frame.shape[:2]\n",
    "        nb_pixels = height * width\n",
    "        nb_blacks = nb_pixels - cv2.countNonZero(frame)\n",
    "        return nb_blacks / nb_pixels\n",
    "\n",
    "    @staticmethod\n",
    "    def find_best_threshold(eye_frame):\n",
    "        \"\"\"Calculates the optimal threshold to binarize the\n",
    "        frame for the given eye.\n",
    "\n",
    "        Argument:\n",
    "            eye_frame (numpy.ndarray): Frame of the eye to be analyzed\n",
    "        \"\"\"\n",
    "        average_iris_size = 0.48\n",
    "        trials = {}\n",
    "\n",
    "        for threshold in range(5, 100, 5):\n",
    "            iris_frame = Pupil.image_processing(eye_frame, threshold)\n",
    "            trials[threshold] = Calibration.iris_size(iris_frame)\n",
    "\n",
    "        best_threshold, iris_size = min(trials.items(), key=(lambda p: abs(p[1] - average_iris_size)))\n",
    "        return best_threshold\n",
    "\n",
    "    def evaluate(self, eye_frame, side):\n",
    "        \"\"\"Improves calibration by taking into consideration the\n",
    "        given image.\n",
    "\n",
    "        Arguments:\n",
    "            eye_frame (numpy.ndarray): Frame of the eye\n",
    "            side: Indicates whether it's the left eye (0) or the right eye (1)\n",
    "        \"\"\"\n",
    "        threshold = self.find_best_threshold(eye_frame)\n",
    "\n",
    "        if side == 0:\n",
    "            self.thresholds_left.append(threshold)\n",
    "        elif side == 1:\n",
    "            self.thresholds_right.append(threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e25819b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Eye(object):\n",
    "    \"\"\"\n",
    "    This class creates a new frame to isolate the eye and\n",
    "    initiates the pupil detection.\n",
    "    \"\"\"\n",
    "\n",
    "    LEFT_EYE_POINTS = [36, 37, 38, 39, 40, 41]\n",
    "    RIGHT_EYE_POINTS = [42, 43, 44, 45, 46, 47]\n",
    "\n",
    "    def __init__(self, original_frame, landmarks, side, calibration):\n",
    "        self.frame = None\n",
    "        self.origin = None\n",
    "        self.center = None\n",
    "        self.pupil = None\n",
    "        self.landmark_points = None\n",
    "\n",
    "        self._analyze(original_frame, landmarks, side, calibration)\n",
    "\n",
    "    @staticmethod\n",
    "    def _middle_point(p1, p2):\n",
    "        \"\"\"Returns the middle point (x,y) between two points\n",
    "\n",
    "        Arguments:\n",
    "            p1 (dlib.point): First point\n",
    "            p2 (dlib.point): Second point\n",
    "        \"\"\"\n",
    "        x = int((p1.x + p2.x) / 2)\n",
    "        y = int((p1.y + p2.y) / 2)\n",
    "        return (x, y)\n",
    "\n",
    "    def _isolate(self, frame, landmarks, points):\n",
    "        \"\"\"Isolate an eye, to have a frame without other part of the face.\n",
    "\n",
    "        Arguments:\n",
    "            frame (numpy.ndarray): Frame containing the face\n",
    "            landmarks (dlib.full_object_detection): Facial landmarks for the face region\n",
    "            points (list): Points of an eye (from the 68 Multi-PIE landmarks)\n",
    "        \"\"\"\n",
    "        region = np.array([(landmarks.part(point).x, landmarks.part(point).y) for point in points])\n",
    "        region = region.astype(np.int32)\n",
    "        self.landmark_points = region\n",
    "\n",
    "        # Applying a mask to get only the eye\n",
    "        height, width = frame.shape[:2]\n",
    "        black_frame = np.zeros((height, width), np.uint8)\n",
    "        mask = np.full((height, width), 255, np.uint8)\n",
    "        cv2.fillPoly(mask, [region], (0, 0, 0))\n",
    "        eye = cv2.bitwise_not(black_frame, frame.copy(), mask=mask)\n",
    "\n",
    "        # Cropping on the eye\n",
    "        margin = 5\n",
    "        min_x = np.min(region[:, 0]) - margin\n",
    "        max_x = np.max(region[:, 0]) + margin\n",
    "        min_y = np.min(region[:, 1]) - margin\n",
    "        max_y = np.max(region[:, 1]) + margin\n",
    "\n",
    "        self.frame = eye[min_y:max_y, min_x:max_x]\n",
    "        self.origin = (min_x, min_y)\n",
    "\n",
    "        height, width = self.frame.shape[:2]\n",
    "        self.center = (width / 2, height / 2)\n",
    "\n",
    "    def _blinking_ratio(self, landmarks, points):\n",
    "        \"\"\"Calculates a ratio that can indicate whether an eye is closed or not.\n",
    "        It's the division of the width of the eye, by its height.\n",
    "\n",
    "        Arguments:\n",
    "            landmarks (dlib.full_object_detection): Facial landmarks for the face region\n",
    "            points (list): Points of an eye (from the 68 Multi-PIE landmarks)\n",
    "\n",
    "        Returns:\n",
    "            The computed ratio\n",
    "        \"\"\"\n",
    "        left = (landmarks.part(points[0]).x, landmarks.part(points[0]).y)\n",
    "        right = (landmarks.part(points[3]).x, landmarks.part(points[3]).y)\n",
    "        top = self._middle_point(landmarks.part(points[1]), landmarks.part(points[2]))\n",
    "        bottom = self._middle_point(landmarks.part(points[5]), landmarks.part(points[4]))\n",
    "\n",
    "        eye_width = math.hypot((left[0] - right[0]), (left[1] - right[1]))\n",
    "        eye_height = math.hypot((top[0] - bottom[0]), (top[1] - bottom[1]))\n",
    "\n",
    "        try:\n",
    "            ratio = eye_width / eye_height\n",
    "        except ZeroDivisionError:\n",
    "            ratio = None\n",
    "\n",
    "        return ratio\n",
    "\n",
    "    def _analyze(self, original_frame, landmarks, side, calibration):\n",
    "        \"\"\"Detects and isolates the eye in a new frame, sends data to the calibration\n",
    "        and initializes Pupil object.\n",
    "\n",
    "        Arguments:\n",
    "            original_frame (numpy.ndarray): Frame passed by the user\n",
    "            landmarks (dlib.full_object_detection): Facial landmarks for the face region\n",
    "            side: Indicates whether it's the left eye (0) or the right eye (1)\n",
    "            calibration (calibration.Calibration): Manages the binarization threshold value\n",
    "        \"\"\"\n",
    "        if side == 0:\n",
    "            points = self.LEFT_EYE_POINTS\n",
    "        elif side == 1:\n",
    "            points = self.RIGHT_EYE_POINTS\n",
    "        else:\n",
    "            return\n",
    "\n",
    "        self.blinking = self._blinking_ratio(landmarks, points)\n",
    "        self._isolate(original_frame, landmarks, points)\n",
    "\n",
    "        if not calibration.is_complete():\n",
    "            calibration.evaluate(self.frame, side)\n",
    "\n",
    "        threshold = calibration.threshold(side)\n",
    "        self.pupil = Pupil(self.frame, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7636ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pupil(object):\n",
    "    \"\"\"\n",
    "    This class detects the iris of an eye and estimates\n",
    "    the position of the pupil\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eye_frame, threshold):\n",
    "        self.iris_frame = None\n",
    "        self.threshold = threshold\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "        self.detect_iris(eye_frame)\n",
    "\n",
    "    @staticmethod\n",
    "    def image_processing(eye_frame, threshold):\n",
    "        \"\"\"Performs operations on the eye frame to isolate the iris\n",
    "\n",
    "        Arguments:\n",
    "            eye_frame (numpy.ndarray): Frame containing an eye and nothing else\n",
    "            threshold (int): Threshold value used to binarize the eye frame\n",
    "\n",
    "        Returns:\n",
    "            A frame with a single element representing the iris\n",
    "        \"\"\"\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        new_frame = cv2.bilateralFilter(eye_frame, 10, 15, 15)\n",
    "        new_frame = cv2.erode(new_frame, kernel, iterations=3)\n",
    "        new_frame = cv2.threshold(new_frame, threshold, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "        return new_frame\n",
    "\n",
    "    def detect_iris(self, eye_frame):\n",
    "        \"\"\"Detects the iris and estimates the position of the iris by\n",
    "        calculating the centroid.\n",
    "\n",
    "        Arguments:\n",
    "            eye_frame (numpy.ndarray): Frame containing an eye and nothing else\n",
    "        \"\"\"\n",
    "        self.iris_frame = self.image_processing(eye_frame, self.threshold)\n",
    "\n",
    "        contours, _ = cv2.findContours(self.iris_frame, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)[-2:]\n",
    "        contours = sorted(contours, key=cv2.contourArea)\n",
    "\n",
    "        try:\n",
    "            moments = cv2.moments(contours[-2])\n",
    "            self.x = int(moments['m10'] / moments['m00'])\n",
    "            self.y = int(moments['m01'] / moments['m00'])\n",
    "        except (IndexError, ZeroDivisionError):\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e9a90b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sayantanbhattacharya/Desktop/Research_radiology/GazeTracking-master\n"
     ]
    }
   ],
   "source": [
    "path=os.getcwd()\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cefbe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GazeTracking(object):\n",
    "    \"\"\"\n",
    "    This class tracks the user's gaze.\n",
    "    It provides useful information like the position of the eyes\n",
    "    and pupils and allows to know if the eyes are open or closed\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.frame = None\n",
    "        self.eye_left = None\n",
    "        self.eye_right = None\n",
    "        self.calibration = Calibration()\n",
    "\n",
    "        # _face_detector is used to detect faces\n",
    "        self._face_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "        # _predictor is used to get facial landmarks of a given face\n",
    "        #cwd = os.path.join(os.getcwd())\n",
    "        cwd = os.path.abspath(os.path.dirname(\"__file__\"))\n",
    "        model_path = os.path.join(cwd, '/Users/sayantanbhattacharya/Desktop/Research_radiology/GazeTracking-master/trained_models/shape_predictor_68_face_landmarks.dat')\n",
    "        self._predictor = dlib.shape_predictor(model_path)\n",
    "\n",
    "    @property\n",
    "    def pupils_located(self):\n",
    "        \"\"\"Check that the pupils have been located\"\"\"\n",
    "        try:\n",
    "            int(self.eye_left.pupil.x)\n",
    "            int(self.eye_left.pupil.y)\n",
    "            int(self.eye_right.pupil.x)\n",
    "            int(self.eye_right.pupil.y)\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def _analyze(self):\n",
    "        \"\"\"Detects the face and initialize Eye objects\"\"\"\n",
    "        frame = cv2.cvtColor(self.frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = self._face_detector(frame)\n",
    "\n",
    "        try:\n",
    "            landmarks = self._predictor(frame, faces[0])\n",
    "            self.eye_left = Eye(frame, landmarks, 0, self.calibration)\n",
    "            self.eye_right = Eye(frame, landmarks, 1, self.calibration)\n",
    "\n",
    "        except IndexError:\n",
    "            self.eye_left = None\n",
    "            self.eye_right = None\n",
    "\n",
    "    def refresh(self, frame):\n",
    "        \"\"\"Refreshes the frame and analyzes it.\n",
    "\n",
    "        Arguments:\n",
    "            frame (numpy.ndarray): The frame to analyze\n",
    "        \"\"\"\n",
    "        self.frame = frame\n",
    "        self._analyze()\n",
    "\n",
    "    def pupil_left_coords(self):\n",
    "        \"\"\"Returns the coordinates of the left pupil\"\"\"\n",
    "        if self.pupils_located:\n",
    "            x = self.eye_left.origin[0] + self.eye_left.pupil.x\n",
    "            y = self.eye_left.origin[1] + self.eye_left.pupil.y\n",
    "            return (x, y)\n",
    "\n",
    "    def pupil_right_coords(self):\n",
    "        \"\"\"Returns the coordinates of the right pupil\"\"\"\n",
    "        if self.pupils_located:\n",
    "            x = self.eye_right.origin[0] + self.eye_right.pupil.x\n",
    "            y = self.eye_right.origin[1] + self.eye_right.pupil.y\n",
    "            return (x, y)\n",
    "\n",
    "    def horizontal_ratio(self):\n",
    "        \"\"\"Returns a number between 0.0 and 1.0 that indicates the\n",
    "        horizontal direction of the gaze. The extreme right is 0.0,\n",
    "        the center is 0.5 and the extreme left is 1.0\n",
    "        \"\"\"\n",
    "        if self.pupils_located:\n",
    "            pupil_left = self.eye_left.pupil.x / (self.eye_left.center[0] * 2 - 10)\n",
    "            pupil_right = self.eye_right.pupil.x / (self.eye_right.center[0] * 2 - 10)\n",
    "            return (pupil_left + pupil_right) / 2\n",
    "\n",
    "    def vertical_ratio(self):\n",
    "        \"\"\"Returns a number between 0.0 and 1.0 that indicates the\n",
    "        vertical direction of the gaze. The extreme top is 0.0,\n",
    "        the center is 0.5 and the extreme bottom is 1.0\n",
    "        \"\"\"\n",
    "        if self.pupils_located:\n",
    "            pupil_left = self.eye_left.pupil.y / (self.eye_left.center[1] * 2 - 10)\n",
    "            pupil_right = self.eye_right.pupil.y / (self.eye_right.center[1] * 2 - 10)\n",
    "            return (pupil_left + pupil_right) / 2\n",
    "\n",
    "    def is_right(self):\n",
    "        \"\"\"Returns true if the user is looking to the right\"\"\"\n",
    "        if self.pupils_located:\n",
    "            return self.horizontal_ratio() <= 0.35\n",
    "\n",
    "    def is_left(self):\n",
    "        \"\"\"Returns true if the user is looking to the left\"\"\"\n",
    "        if self.pupils_located:\n",
    "            return self.horizontal_ratio() >= 0.65\n",
    "\n",
    "    def is_center(self):\n",
    "        \"\"\"Returns true if the user is looking to the center\"\"\"\n",
    "        if self.pupils_located:\n",
    "            return self.is_right() is not True and self.is_left() is not True\n",
    "\n",
    "    def is_blinking(self):\n",
    "        \"\"\"Returns true if the user closes his eyes\"\"\"\n",
    "        if self.pupils_located:\n",
    "            blinking_ratio = (self.eye_left.blinking + self.eye_right.blinking) / 2\n",
    "            return blinking_ratio > 3.8\n",
    "\n",
    "    def annotated_frame(self):\n",
    "        \"\"\"Returns the main frame with pupils highlighted\"\"\"\n",
    "        frame = self.frame.copy()\n",
    "\n",
    "        if self.pupils_located:\n",
    "            color = (0, 255, 0)\n",
    "            x_left, y_left = self.pupil_left_coords()\n",
    "            x_right, y_right = self.pupil_right_coords()\n",
    "            cv2.line(frame, (x_left - 5, y_left), (x_left + 5, y_left), color)\n",
    "            cv2.line(frame, (x_left, y_left - 5), (x_left, y_left + 5), color)\n",
    "            cv2.line(frame, (x_right - 5, y_right), (x_right + 5, y_right), color)\n",
    "            cv2.line(frame, (x_right, y_right - 5), (x_right, y_right + 5), color)\n",
    "\n",
    "        return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "260cd722",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to open /Users/sayantanbhattacharya/Desktop/Research_radiology/GazeTracking-master/trained_models/shape_predictor_68_face_landmarks.dat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gaze \u001b[38;5;241m=\u001b[39m \u001b[43mGazeTracking\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mGazeTracking.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m cwd \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     20\u001b[0m model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cwd, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/sayantanbhattacharya/Desktop/Research_radiology/GazeTracking-master/trained_models/shape_predictor_68_face_landmarks.dat\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predictor \u001b[38;5;241m=\u001b[39m \u001b[43mdlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape_predictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to open /Users/sayantanbhattacharya/Desktop/Research_radiology/GazeTracking-master/trained_models/shape_predictor_68_face_landmarks.dat"
     ]
    }
   ],
   "source": [
    "gaze = GazeTracking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a067e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gaze = GazeTracking()\n",
    "webcam = cv2.VideoCapture(0)\n",
    "count=0\n",
    "gaze_data=[]\n",
    "\n",
    "while True:\n",
    "    # We get a new frame from the webcam\n",
    "    _, frame = webcam.read()\n",
    "\n",
    "    # We send this frame to GazeTracking to analyze it\n",
    "    gaze.refresh(frame)\n",
    "\n",
    "    frame = gaze.annotated_frame()\n",
    "    text = \"\"\n",
    "\n",
    "    if gaze.is_blinking():\n",
    "        text = \"Blinking\"\n",
    "    elif gaze.is_right():\n",
    "        text = \"Looking right\"\n",
    "    elif gaze.is_left():\n",
    "        text = \"Looking left\"\n",
    "    elif gaze.is_center():\n",
    "        text = \"Looking center\"\n",
    "\n",
    "    cv2.putText(frame, text, (90, 60), cv2.FONT_HERSHEY_DUPLEX, 1.6, (147, 58, 31), 2)\n",
    "\n",
    "    left_pupil = gaze.pupil_left_coords()\n",
    "    right_pupil = gaze.pupil_right_coords()\n",
    "    gaze_data.append({'frame':count,'left_pupil':left_pupil,'right_pupil':right_pupil})\n",
    "    cv2.putText(frame, \"Left pupil:  \" + str(left_pupil), (90, 130), cv2.FONT_HERSHEY_DUPLEX, 0.9, (147, 58, 31), 1)\n",
    "    cv2.putText(frame, \"Right pupil: \" + str(right_pupil), (90, 165), cv2.FONT_HERSHEY_DUPLEX, 0.9, (147, 58, 31), 1)\n",
    "\n",
    "    cv2.imshow(\"Demo\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "df=pd.DataFrame.from_dict(gaze_data)\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "df.to_csv('out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c7e4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10386a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b26f6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = napari.view_image(vol, name='MRI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83205ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "\n",
    "vol = io.imread(\"https://s3.amazonaws.com/assets.datacamp.com/blog_assets/attention-mri.tif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51b42ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol.shape\n",
    "vol_a=np.swapaxes(vol,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34f6215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_keymap_conflicts(new_keys_set):\n",
    "    for prop in plt.rcParams:\n",
    "        if prop.startswith('keymap.'):\n",
    "            keys = plt.rcParams[prop]\n",
    "            remove_list = set(keys) & new_keys_set\n",
    "            for key in remove_list:\n",
    "                keys.remove(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e523d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(19680801)\n",
    "\n",
    "class IndexTracker:\n",
    "    def __init__(self, ax, X):\n",
    "        self.ax = ax\n",
    "        ax.set_title(\"use buttons j and k to navigate images\")\n",
    "\n",
    "        self.X = X\n",
    "        rows, cols, self.slices = X.shape\n",
    "        self.ind = self.slices//2\n",
    "\n",
    "        self.im = ax.imshow(self.X[:, :, self.ind])\n",
    "        self.update()\n",
    "\n",
    "    def on_scroll(self, event):\n",
    "        print(\"%s %s\" % (event.button, event.step))\n",
    "        if event.button == 'up':\n",
    "            self.ind = (self.ind + 1) % self.slices\n",
    "        else:\n",
    "            self.ind = (self.ind - 1) % self.slices\n",
    "        self.update()\n",
    "\n",
    "    def update(self):\n",
    "        self.im.set_data(self.X[:, :, self.ind])\n",
    "        self.ax.set_ylabel('slice %s' % self.ind)\n",
    "        self.im.axes.figure.canvas.draw()\n",
    "        \n",
    "    def process_key(self, event):\n",
    "        if event.key == 'j':\n",
    "            self.previous_slice(ax)\n",
    "        elif event.key == 'k':\n",
    "            self.next_slice(ax)\n",
    "        self.update()\n",
    "\n",
    "    def previous_slice(self, ax):\n",
    "        self.ind = (self.ind - 1) % self.slices\n",
    "\n",
    "    def next_slice(self, ax):\n",
    "        self.ind = (self.ind + 1) % self.slices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdbbbfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b85fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
